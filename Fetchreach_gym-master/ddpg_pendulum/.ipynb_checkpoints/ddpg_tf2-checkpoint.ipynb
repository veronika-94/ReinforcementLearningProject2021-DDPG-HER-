{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9524d2df0ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCriticNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'buffer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from buffer import ReplayBuffer\n",
    "from networks import ActorNetwork, CriticNetwork\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,\n",
    "            gamma=0.99, n_actions=2, max_size=1000000, tau=0.005, \n",
    "            fc1=400, fc2=300, batch_size=64, noise=0.1):\n",
    "    #alpha-->learning rate actor net, beta = lr critic network\n",
    "    #gamma discount factor, tau for soft update target nets \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.noise = noise\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.min_action = env.action_space.low[0]\n",
    "        \n",
    "        self.actor = ActorNetwork(n_actions=n_actions, name='actor')\n",
    "        self.critic = CriticNetwork(n_actions=n_actions, name='critic')\n",
    "        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor')\n",
    "        self.target_critic = CriticNetwork(name='target_critic')\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha)) #weights are soft updated, just a tf need\n",
    "        self.target_critic.compile(optimizer=Adam(learning_rate=beta)) #weights ////////\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None: #hard copy \n",
    "            tau = self.tau # that is equal 1\n",
    "\n",
    "        weights = [] #for tager actor net\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "\n",
    "        weights = [] #for target critic net\n",
    "        targets = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_weights(self.actor.checkpoint_file)\n",
    "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
    "        self.critic.save_weights(self.critic.checkpoint_file)\n",
    "        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_weights(self.actor.checkpoint_file)\n",
    "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
    "        self.critic.load_weights(self.critic.checkpoint_file)\n",
    "        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n",
    "\n",
    "    def choose_action(self, observation, evaluate=False):\n",
    "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        actions = self.actor(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions],\n",
    "                    mean=0.0, stddev=self.noise)\n",
    "        # note that if the environment has an action > 1, we have to multiply by\n",
    "        # max action at some point\n",
    "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape: #critic update\n",
    "            target_actions = self.target_actor(states_)\n",
    "            critic_value_ = tf.squeeze(self.target_critic(\n",
    "                                states_, target_actions), 1)\n",
    "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
    "            target = reward + self.gamma*critic_value_*(1-done)\n",
    "            critic_loss = keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        critic_network_gradient = tape.gradient(critic_loss,\n",
    "                                            self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(\n",
    "            critic_network_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape: #actor update\n",
    "            new_policy_actions = self.actor(states)\n",
    "            actor_loss = -self.critic(states, new_policy_actions) #gradient ascent\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, \n",
    "                                    self.actor.trainable_variables)#\n",
    "        #gradient of actorloss wrt trainable variables\n",
    "        self.actor.optimizer.apply_gradients(zip(\n",
    "            actor_network_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
